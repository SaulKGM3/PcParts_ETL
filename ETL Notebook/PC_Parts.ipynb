{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec63d33-7c19-4a2f-b310-029dd25545fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "47a07903-29ff-4099-a2fb-23ff5b8f5cbd",
   "metadata": {},
   "source": [
    "# ETL Process for PC PARTS Dataset\n",
    "\n",
    "This notebook performs a *semi automated ETL* process for sales, cost, and inventory data stored in the `PCPARTS_DB` database, with human validation. The database contains a mix of real and synthetic data. Once processed, the data is loaded into SQL Server and ready for use.\n",
    "\n",
    "## Performed Steps\n",
    "\n",
    "1. **Reading datasets** from local sources or previous transformations.\n",
    "2. **Connecting to SQL Server** using SQLAlchemy and ODBC.\n",
    "3. **Uploading dataframes** to the database:\n",
    "   - `df_ventas`\n",
    "   - `df_costos`\n",
    "   - `df_inventario`\n",
    "   - `inventario_agregado`\n",
    "   - `ventas_agregado`\n",
    "\n",
    "> *Connection credentials have been censored for security purposes.*\n",
    "\n",
    "> *This an ongoing project building and further refinement/improvement is pending to be implemented*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fb6245-b3dc-4724-a7a8-8d7bda9cbd18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12f76c57-71d4-4860-8698-849acff5f80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruta de los archivos CSV\n",
    "ruta = r\"C://Users//PC USER//PC PART DB//Current month\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "182a0e92-7ca6-4675-a876-3443810220cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inventory\n",
      "\n",
      "  Fechainventario       Sku  Stockinicial  Stockrecibido  Stockvendido  \\\n",
      "0      2023-04-01  sku_0057            55             32            39   \n",
      "1      2023-04-08  sku_0057            93             45           102   \n",
      "2      2023-04-15  sku_0057            40             50            39   \n",
      "3      2023-04-22  sku_0057            36             15            29   \n",
      "4      2023-04-01  sku_0090            29              2             5   \n",
      "\n",
      "   Stockfinal  Descontinuado  Tiendaid  \n",
      "0          48          False  tienda_1  \n",
      "1          36          False  tienda_1  \n",
      "2          51          False  tienda_1  \n",
      "3          22          False  tienda_1  \n",
      "4          26          False  tienda_1   \n",
      "\n",
      "Sales\n",
      "\n",
      "  Fechaventa       Sku  Preciounitario  Unidadesvendidas   Ingresos  Tiendaid\n",
      "0 2023-04-03  sku_0053         9875.22                26  256755.72  tienda_1\n",
      "1 2023-04-03  sku_0085         9204.58                 2   18409.16  tienda_1\n",
      "2 2023-04-03  sku_0048         4178.46                11   45963.06  tienda_1\n",
      "3 2023-04-03  sku_0086         8580.67                21  180194.07  tienda_1\n",
      "4 2023-04-03  sku_0095         8520.44                 9   76683.96  tienda_1 \n",
      "\n",
      "Costs\n",
      "\n",
      "  Fechacosto       Sku  Costounitario  Unidadesadquiridas  Gastototal  \\\n",
      "0 2023-04-03  sku_0000        8773.15                  32   280740.80   \n",
      "1 2023-04-03  sku_0001        4933.40                  40   197336.00   \n",
      "2 2023-04-03  sku_0002        7429.87                  43   319484.41   \n",
      "3 2023-04-03  sku_0003        5499.34                  33   181478.22   \n",
      "4 2023-04-03  sku_0004        7303.11                   0        0.00   \n",
      "\n",
      "   Tiendaid  \n",
      "0  tienda_1  \n",
      "1  tienda_1  \n",
      "2  tienda_1  \n",
      "3  tienda_1  \n",
      "4  tienda_1   \n",
      "\n",
      "Aggregated Inventory\n",
      "\n",
      "  Fechainventario  Tiendaid       Sku  Stockinicial  Stockfinal\n",
      "0      2023-04-30  tienda_1  sku_0000           232          88\n",
      "1      2023-04-30  tienda_1  sku_0008           200          86\n",
      "2      2023-04-30  tienda_1  sku_0022           213         172\n",
      "3      2023-04-30  tienda_1  sku_0025           183          65\n",
      "4      2023-04-30  tienda_1  sku_0027           529         404 \n",
      "\n",
      "Aggregated Sales\n",
      "\n",
      "  Fechaventa  Tiendaid       Sku  Unidadesvendidas   Ingresos\n",
      "0 2023-04-30  tienda_1  sku_0000                19  112041.42\n",
      "1 2023-04-30  tienda_1  sku_0002                13  117690.30\n",
      "2 2023-04-30  tienda_1  sku_0003                50  255131.12\n",
      "3 2023-04-30  tienda_1  sku_0005                 8   71868.56\n",
      "4 2023-04-30  tienda_1  sku_0006                40  266505.46 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "\n",
    "\n",
    "# Dictionary to store dataframe\n",
    "dataframes = {}\n",
    "\n",
    "# Function to normalize text\n",
    "def normalizar(texto):\n",
    "    return unicodedata.normalize('NFKD', str(texto)).encode('ASCII', 'ignore').decode('utf-8').lower().strip() # Esay/fast to clean\n",
    "\n",
    "# Loading and cleaning files name \n",
    "for archivo in os.listdir(ruta):\n",
    "    if archivo.endswith('.csv'): # Only select .csv files avoiding anything else in folder\n",
    "        nombre_df = archivo.replace('.csv', '').replace('-', '').replace(' ', '').replace('_', '') # Reduced variability in format\n",
    "        nombre_df = normalizar(nombre_df).capitalize() # To avoid reserved words in SQL\n",
    "\n",
    "        df = pd.read_csv(os.path.join(ruta, archivo))\n",
    "\n",
    "# Columns name cleaning\n",
    "        nuevas_columnas = []\n",
    "        for col in df.columns:\n",
    "            col = normalizar(col)\n",
    "            col = col.replace(' ', '').replace('-', '').replace('_', '')\n",
    "            col = col.capitalize()\n",
    "            \n",
    "            if col == \"Fecha\": # Differentiates columns with the same name across multiple tables\n",
    "                if \"inventario\" in nombre_df:\n",
    "                    col = f\"{col}inventario\"\n",
    "                elif \"venta\" in nombre_df:\n",
    "                    col = f\"{col}venta\"\n",
    "                elif \"costo\" in nombre_df:\n",
    "                    col = f\"{col}costo\"\n",
    "                \n",
    "            nuevas_columnas.append(col)\n",
    "        df.columns = nuevas_columnas\n",
    "\n",
    "# Columns cleaning\n",
    "\n",
    "        for col in df.select_dtypes(include='object').columns:\n",
    "            df[col] = df[col].apply(normalizar) # Resulting type in pandas is object\n",
    "            if col.startswith(\"Fecha\"): # Converts date columns to datetime format\n",
    "                df[col] = pd.to_datetime(df[col], errors='coerce')  \n",
    "\n",
    "        dataframes[nombre_df] = df  \n",
    "        \n",
    "# Groups by table type\n",
    "ventas = []\n",
    "inventario = []\n",
    "costos = []\n",
    "\n",
    "for nombre, df in dataframes.items():\n",
    "    if \"inventario\" in nombre:\n",
    "        inventario.append(df)\n",
    "    elif \"venta\" in nombre:\n",
    "        ventas.append(df)\n",
    "    elif \"costo\" in nombre:\n",
    "        costos.append(df)\n",
    "\n",
    "# Append tables\n",
    "df_ventas = pd.concat(ventas, ignore_index=True) # Data from different sources are consolidated in the main table\n",
    "df_inventario = pd.concat(inventario, ignore_index=True)\n",
    "df_costos = pd.concat(costos, ignore_index=True)\n",
    "\n",
    "TablasT = [ventas, inventario, costos]\n",
    "\n",
    "for z in TablasT:\n",
    "    for w in z:\n",
    "        if w.isnull().sum().sum()>0:\n",
    "            print(f\"NULLS IN {w.isnull().sum().sum()}\")\n",
    "\n",
    "\n",
    "# Aggregated Inventory\n",
    "\n",
    "inventario_agregado = df_inventario.groupby([       \n",
    "    pd.Grouper(key='Fechainventario', freq='ME'),\n",
    "    'Tiendaid', 'Sku'\n",
    "]).agg({\n",
    "    'Stockinicial': 'sum',    \n",
    "    'Stockfinal': 'sum'       \n",
    "}).reset_index()  # Resume table for additional insights\n",
    "\n",
    "# Aggregated Sales\n",
    "\n",
    "ventas_agregado = df_ventas.groupby([\n",
    "    pd.Grouper(key='Fechaventa', freq='ME'), \n",
    "    'Tiendaid', 'Sku'\n",
    "]).agg({\n",
    "    'Unidadesvendidas': 'sum',\n",
    "    'Ingresos': 'sum'\n",
    "}).reset_index() # Resume table for additional insights\n",
    "\n",
    "# For human validation before upload\n",
    "\n",
    "print(\"Inventory\\n\")\n",
    "print(df_inventario.head(), \"\\n\") # Supports an efficient data validation\n",
    "\n",
    "print(\"Sales\\n\")\n",
    "print(df_ventas.head(), \"\\n\")\n",
    "\n",
    "print(\"Costs\\n\")\n",
    "print(df_costos.head(), \"\\n\")\n",
    "\n",
    "print(\"Aggregated Inventory\\n\")\n",
    "print(inventario_agregado.head(), \"\\n\")\n",
    "\n",
    "print(\"Aggregated Sales\\n\")\n",
    "print(ventas_agregado.head(), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3c70c0-2558-4425-a06d-8f171e61f05d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f9fbca-683a-4408-89f9-d11a25587456",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "51f99e52-c038-4c66-895c-14436c8f8dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload to SQL\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "from urllib.parse import quote_plus\n",
    "\n",
    "cadena_odbc = (\n",
    "    \"*********\"\n",
    ")\n",
    "\n",
    "cadena_conexion = \"*********\" + quote_plus(cadena_odbc)\n",
    "engine = create_engine(cadena_conexion)\n",
    "\n",
    "df_ventas.to_sql('df_ventas', engine, if_exists='append', index=False)\n",
    "df_costos.to_sql('df_costos', engine, if_exists='append', index=False)\n",
    "df_inventario.to_sql('df_inventario', engine, if_exists='append', index=False)\n",
    "inventario_agregado.to_sql('inventario_agregado', engine, if_exists='append', index=False)\n",
    "ventas_agregado.to_sql('ventas_agregado', engine, if_exists='append', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435e56d8-8003-4881-b991-41bcbb68f8ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
